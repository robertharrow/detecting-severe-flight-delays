{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Major Flight Delays\n",
    "\n",
    "Our client, FlightChicken, would like a model that predicts whether a flight will experience a major delay. Delays can cause a major disruption to travel plans, especially if they cause a person to miss their connecting flight. FlightChicken would like to give their users a heads up about potential travel disruptions like this.\n",
    "\n",
    "This is a major undertaking as there are hundreds of airlines and thousands of airports in the United States alone. That's why FlightChicken would like to launch with just an MVP to prove our their concept. This MVP should support major US airports and 8 of the most popular airlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pandas import Timestamp\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "import airportsdata\n",
    "from pytz import timezone\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "MVP should support:\n",
    "\n",
    "* [Top 8 US Airlines](https://www.statista.com/statistics/250577/domestic-market-share-of-leading-us-airlines/)\n",
    " * American Airlines\n",
    " * Delta Air Lines\n",
    " * United Airlines\n",
    " * Southwest Airlines\n",
    " * Alaska Airlines\n",
    " * JetBlue Airways\n",
    " * Spirit\n",
    " * SkyWest\n",
    "* [Large and medium airport hubs](https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger/media/cy20-commercial-service-enplanements.pdf)\n",
    " * \"The term hub is used by the FAA to identify very busy commercial service airports. Large hubs are the airports that each account for at least one percent of total U.S. passenger enplanements.\"\n",
    " * In 2020 these accounted for 84% of all enplanements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "To complete this project, we will be using data from several sources.\n",
    "\n",
    "1. **Bureau of Transportation Statistics: Carrier On-Time Performence Database.** This database contains scheduled and actual departure and arrival times reported by certified U.S. air carriers that account for at least one percent of domestic scheduled passenger revenues. The data is collected by the Office of Airline Information, Bureau of Transportation Statistics (BTS).\n",
    "2. **National Oceanic and Atmospheric Administration (NOAA): Daily Weather Summaries:** Data on select weather conditions at airprots collected by weather stations.\n",
    "3. **Timezone for Each Airport by StackOverflow user hroptatyr:** This data will allow us to convert our timedata to UTC and make it easier to work with. [Link](https://raw.githubusercontent.com/hroptatyr/dateutils/tzmaps/iata.tzmap)\n",
    "\n",
    "Additionally, to link data from NOAA to each airport, I manually looked up the weather station for all airports relevant in this project. This data was can be found in FILE PATH. To reproduce, go to [Climate Data Online Search](https://www.ncei.noaa.gov/cdo-web/search) and make the following selections:\n",
    "\n",
    "1. Select Weather Observation Type/Dataset: Daily Summaries\n",
    "2. Select Date Range: 2018-01-01 to 2021-12-31\n",
    "3. Search For: Stations\n",
    "4. Enter a Search Term: enter the city and state of the airport plus the term 'airport'. e.g. Atlanta, GA airport\n",
    "5. Hit 'Search'\n",
    "6. On the results page, find the closest/most relevant weather station. In the example of \"Atlanta, GA airport\" you would select 'Atlanta Hartsfield Jackson International Airport\". Hit 'Add to Cart'. **On the results page, make note of the Station ID. This is what will serve as the key for linking BTS data with weather data.**\n",
    "7. Repeat for every airport.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation & Cleaning\n",
    "First, we need to load all our data into pandas so that we can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carrier On-Time Performence Database\n",
    "This data can only be downloaded by month, which means it is split among many files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flight_data = glob.glob(os.path.join('data/downloaded/carrier-on-time-performence', \"*.csv\"))\n",
    "carrier_data = pd.concat((pd.read_csv(f) for f in flight_data), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're trying to predict only flight delays not cancellations so we'll remove those from this list\n",
    "carrier_data = carrier_data.loc[carrier_data['CANCELLED'] == 0]\n",
    "carrier_data.drop(['CANCELLED'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YEAR                         0\n",
       "MONTH                        0\n",
       "DAY_OF_MONTH                 0\n",
       "DAY_OF_WEEK                  0\n",
       "FL_DATE                      0\n",
       "MKT_CARRIER                  0\n",
       "MKT_CARRIER_FL_NUM           0\n",
       "OP_CARRIER                   0\n",
       "TAIL_NUM                     0\n",
       "OP_CARRIER_FL_NUM            0\n",
       "ORIGIN                       0\n",
       "DEST                         0\n",
       "CRS_DEP_TIME                 0\n",
       "DEP_DELAY                    0\n",
       "DEP_DELAY_NEW                0\n",
       "CRS_ARR_TIME                 0\n",
       "ARR_DELAY_NEW            19033\n",
       "CRS_ELAPSED_TIME             0\n",
       "DISTANCE                     0\n",
       "CARRIER_DELAY          5864061\n",
       "WEATHER_DELAY          5864061\n",
       "NAS_DELAY              5864061\n",
       "SECURITY_DELAY         5864061\n",
       "LATE_AIRCRAFT_DELAY    5864061\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping Columns with a lot of missing info\n",
    "carrier_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY']\n",
    "carrier_data.drop(cols_to_drop, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_data = carrier_data[np.isfinite(carrier_data['ARR_DELAY_NEW'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing In Additional Data on Our Airports\n",
    "Next, we'll use the airportsdata package to load in some additional details for each airport (like its location, timezone, and elevation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = airportsdata.load('IATA')\n",
    "airports_list = []\n",
    "lat_list = []\n",
    "lon_list = []\n",
    "elevation_list = []\n",
    "tz_list = []\n",
    "\n",
    "for key in airports.keys():\n",
    "    airport = airports[key]['iata']\n",
    "    lat = airports[key]['lat']\n",
    "    lon = airports[key]['lon']\n",
    "    elevation = airports[key]['elevation']\n",
    "    tz = airports[key]['tz']\n",
    "    airports_list.append(airport)\n",
    "    lat_list.append(lat)\n",
    "    lon_list.append(lon)\n",
    "    elevation_list.append(elevation)\n",
    "    tz_list.append(tz)\n",
    "\n",
    "airport_locations = pd.DataFrame(\n",
    "    {'Airport': airports_list,\n",
    "     'Latitude': lat_list,\n",
    "     'Longitude': lon_list,\n",
    "     'Timezone': tz_list,\n",
    "     'Elevation': elevation_list,\n",
    "    })\n",
    "\n",
    "airport_locations['lat-long'] = airport_locations['Latitude'].astype(str) + ',' + airport_locations['Longitude'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we bring that information to our main dataframe\n",
    "carrier_data['origin-lat-long'] = carrier_data['ORIGIN'].map(airport_locations.set_index('Airport')['lat-long'])\n",
    "carrier_data['origin-tz'] = carrier_data['ORIGIN'].map(airport_locations.set_index('Airport')['Timezone'])\n",
    "carrier_data['origin-elevation'] = carrier_data['ORIGIN'].map(airport_locations.set_index('Airport')['Elevation'])\n",
    "\n",
    "carrier_data['dest-lat-long'] = carrier_data['DEST'].map(airport_locations.set_index('Airport')['lat-long'])\n",
    "carrier_data['dest-tz'] = carrier_data['DEST'].map(airport_locations.set_index('Airport')['Timezone'])\n",
    "carrier_data['dest-elevation'] = carrier_data['DEST'].map(airport_locations.set_index('Airport')['Elevation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing origin timezones: 13825\n",
      "Missing origin elevations: 13825\n",
      "Missing origin locations: 13825\n",
      "------------------\n",
      "Missing destination timezones: 13810\n",
      "Missing destination  elevations: 13810\n",
      "Missing destination  locations: 13810\n"
     ]
    }
   ],
   "source": [
    "# Check whether we were able to match all records\n",
    "print('Missing origin timezones: {}'.format(carrier_data['origin-tz'].isna().sum()))\n",
    "print('Missing origin elevations: {}'.format(carrier_data['origin-elevation'].isna().sum()))\n",
    "print('Missing origin locations: {}'.format(carrier_data['origin-lat-long'].isna().sum()))\n",
    "print('------------------')\n",
    "print('Missing destination timezones: {}'.format(carrier_data['dest-tz'].isna().sum()))\n",
    "print('Missing destination  elevations: {}'.format(carrier_data['dest-elevation'].isna().sum()))\n",
    "print('Missing destination  locations: {}'.format(carrier_data['dest-lat-long'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like about 13,000 records couldn't be matched. That's not bad considering we have over 7,000,000 record total. We'll simply drop the Null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "carrier_data = carrier_data.dropna(subset=['origin-tz', 'origin-elevation', 'origin-lat-long',\n",
    "                                           'dest-tz', 'dest-elevation', 'dest-lat-long'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time feature we are concerned with is takeoff and landing time.  Our goal is to convert it to a universal UTC time. To do this, we first need to transform it a bit so that it's workable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/13/2021 12:00:00 AM</td>\n",
       "      <td>1825</td>\n",
       "      <td>2140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/13/2021 12:00:00 AM</td>\n",
       "      <td>1210</td>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7/13/2021 12:00:00 AM</td>\n",
       "      <td>2050</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7/13/2021 12:00:00 AM</td>\n",
       "      <td>1840</td>\n",
       "      <td>2140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7/13/2021 12:00:00 AM</td>\n",
       "      <td>840</td>\n",
       "      <td>1145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580719</th>\n",
       "      <td>1/9/2022 12:00:00 AM</td>\n",
       "      <td>1224</td>\n",
       "      <td>1352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580720</th>\n",
       "      <td>1/10/2022 12:00:00 AM</td>\n",
       "      <td>1224</td>\n",
       "      <td>1352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580721</th>\n",
       "      <td>1/11/2022 12:00:00 AM</td>\n",
       "      <td>1224</td>\n",
       "      <td>1352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580722</th>\n",
       "      <td>1/12/2022 12:00:00 AM</td>\n",
       "      <td>1224</td>\n",
       "      <td>1352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580723</th>\n",
       "      <td>1/13/2022 12:00:00 AM</td>\n",
       "      <td>1224</td>\n",
       "      <td>1352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7346524 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       FL_DATE  CRS_DEP_TIME  CRS_ARR_TIME\n",
       "0        7/13/2021 12:00:00 AM          1825          2140\n",
       "2        7/13/2021 12:00:00 AM          1210          1530\n",
       "3        7/13/2021 12:00:00 AM          2050             5\n",
       "4        7/13/2021 12:00:00 AM          1840          2140\n",
       "5        7/13/2021 12:00:00 AM           840          1145\n",
       "...                        ...           ...           ...\n",
       "7580719   1/9/2022 12:00:00 AM          1224          1352\n",
       "7580720  1/10/2022 12:00:00 AM          1224          1352\n",
       "7580721  1/11/2022 12:00:00 AM          1224          1352\n",
       "7580722  1/12/2022 12:00:00 AM          1224          1352\n",
       "7580723  1/13/2022 12:00:00 AM          1224          1352\n",
       "\n",
       "[7346524 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carrier_data[['FL_DATE', 'CRS_DEP_TIME', 'CRS_ARR_TIME']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All times are expressed as an integer in military time. For example, 940 is 9:40am and 1500 is 3:00pm. We would like to first convert it to a string that can be read as 24H time, then combined with the FL_DATE field so that we can have an exact take-off date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First we create a helper function to carry out the transformation\n",
    "def float_to_time(time):\n",
    "    '''\n",
    "    Function takes in an integer representation of time (24-hour format)\n",
    "    and returns a string in proper datetime formatting. Example: 1545 (int) becomes 15:45 (string)\n",
    "    '''\n",
    "    time_str = str(time)\n",
    "    digits = len(time_str)\n",
    "    if digits < 2:\n",
    "        return '00:0' + str(time)\n",
    "    if digits == 2:\n",
    "        return '00:' + str(time)\n",
    "    if digits == 3:\n",
    "        return '0' + time_str[:1] + ':' + time_str[1:]\n",
    "    if digits == 4:\n",
    "        return time_str[:2] + ':' + time_str[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, we apply the function above to transform the CRS_DEP_TIME field\n",
    "carrier_data['CRS_DEP_TIME'] = carrier_data['CRS_DEP_TIME'].apply(float_to_time)\n",
    "carrier_data['CRS_ARR_TIME'] = carrier_data['CRS_ARR_TIME'].apply(float_to_time)\n",
    "# Next, we update the FL_DATE field so that it now contains the proper date AND time of takeoff\n",
    "carrier_data['FL_DATE'] =  pd.to_datetime(carrier_data['FL_DATE'].astype(str) + ' ' + carrier_data['CRS_DEP_TIME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the above expressed in LOCAL time, we will use the timezone data to create an additional element in UTC time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First we need to make our data, which is timezone naive, to timezone aware\n",
    "carrier_data['FL_DATE'] = carrier_data['FL_DATE'].astype('datetime64[ns]')\n",
    "carrier_data['FL_DATE_LOCAL'] = carrier_data.apply(lambda x: x['FL_DATE'].replace(tzinfo=timezone(x['origin-tz'])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to know when the flight will be landing. We have the field CRS_ELAPSED_TIME to see how many minutes are supposed to take between departure and arrival. This is expressed in minutes. We can add that to the UTC FL_DATE and then convert it to a local time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Robert\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:1342: PerformanceWarning: Adding/subtracting object-dtype array to TimedeltaArray not vectorized\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "carrier_data['flight_duration'] = pd.to_timedelta(carrier_data['CRS_ELAPSED_TIME'],'m')\n",
    "carrier_data['FL_ARR_DATE_REL_ORIGIN'] = carrier_data['FL_DATE_LOCAL'] + carrier_data['flight_duration']\n",
    "# And now we convert arrival time and date to a local time\n",
    "carrier_data['FL_ARR_DATE_LOCAL'] = carrier_data.apply(lambda x: x['FL_ARR_DATE_REL_ORIGIN'].tz_convert(x['dest-tz']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airport Congestion\n",
    "One hypothesis is that flight delays can be tied to airport \"traffic\" (or congestion). It stands to reason that if an airline has 100 flights scheduled to take off at 11am, there is a higher chance of delays than if there are only 10 flights scheduled to take off.\n",
    "\n",
    "Moreover, because traffic and delays in the morning can propegate throughout the day congestion before our flight takes off can also play a role.\n",
    "\n",
    "We'll create a set of features that put a number on this congestion.\n",
    "\n",
    "First, we will round takeoff times to the nearest hour. This will make calculations easier. Then, we will createa  dataframe that holds information on airport congestion at every airport, at every hour of the day throughout the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def takeoff_hour_rounder(time):\n",
    "    '''\n",
    "    Function takes in a time and returns time rounded to the \n",
    "    nearest hour by adding a timedelta hour if minute >= 30\n",
    "    '''\n",
    "    return (time.replace(second=0, microsecond=0, minute=0, hour=time.hour)\n",
    "               +timedelta(hours=time.minute//30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "carrier_data['FL_DATE_LOCAL_ROUNDED'] = carrier_data['FL_DATE_LOCAL'].apply(takeoff_hour_rounder)\n",
    "carrier_data['FL_ARR_DATE_LOCAL_ROUNDED'] = carrier_data['FL_ARR_DATE_LOCAL'].apply(takeoff_hour_rounder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We won't be needitn timezone info anymore, so let's remove it\n",
    "def remove_timezone(dt):\n",
    "    # HERE `dt` is a python datetime\n",
    "    # object that used .replace() method\n",
    "    \n",
    "    return dt.replace(tzinfo=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_data['FL_DATE_LOCAL'] = carrier_data['FL_DATE_LOCAL'].apply(remove_timezone)\n",
    "carrier_data['FL_DATE_LOCAL_ROUNDED'] = carrier_data['FL_DATE_LOCAL_ROUNDED'].apply(remove_timezone)\n",
    "carrier_data['FL_ARR_DATE_LOCAL'] = carrier_data['FL_ARR_DATE_LOCAL'].apply(remove_timezone)\n",
    "carrier_data['FL_ARR_DATE_LOCAL_ROUNDED'] = carrier_data['FL_ARR_DATE_LOCAL_ROUNDED'].apply(remove_timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_data['ARR_DAY_OF_WEEK'] = carrier_data['FL_ARR_DATE_LOCAL'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>FL_DATE_LOCAL</th>\n",
       "      <th>FL_ARR_DATE_LOCAL</th>\n",
       "      <th>ARR_DAY_OF_WEEK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAN</td>\n",
       "      <td>DEN</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-07-13 18:25:00</td>\n",
       "      <td>2021-07-13 21:40:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAN</td>\n",
       "      <td>DEN</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-07-13 12:10:00</td>\n",
       "      <td>2021-07-13 15:30:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SAN</td>\n",
       "      <td>DEN</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-07-13 20:50:00</td>\n",
       "      <td>2021-07-14 00:05:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SAN</td>\n",
       "      <td>HNL</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-07-13 18:40:00</td>\n",
       "      <td>2021-07-13 21:40:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SAN</td>\n",
       "      <td>HNL</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-07-13 08:40:00</td>\n",
       "      <td>2021-07-13 11:45:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580719</th>\n",
       "      <td>CMH</td>\n",
       "      <td>DCA</td>\n",
       "      <td>7</td>\n",
       "      <td>2022-01-09 12:24:00</td>\n",
       "      <td>2022-01-09 13:52:00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580720</th>\n",
       "      <td>CMH</td>\n",
       "      <td>DCA</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-10 12:24:00</td>\n",
       "      <td>2022-01-10 13:52:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580721</th>\n",
       "      <td>CMH</td>\n",
       "      <td>DCA</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-11 12:24:00</td>\n",
       "      <td>2022-01-11 13:52:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580722</th>\n",
       "      <td>CMH</td>\n",
       "      <td>DCA</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-01-12 12:24:00</td>\n",
       "      <td>2022-01-12 13:52:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580723</th>\n",
       "      <td>CMH</td>\n",
       "      <td>DCA</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-01-13 12:24:00</td>\n",
       "      <td>2022-01-13 13:52:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7346524 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ORIGIN DEST  DAY_OF_WEEK       FL_DATE_LOCAL   FL_ARR_DATE_LOCAL  \\\n",
       "0          SAN  DEN            2 2021-07-13 18:25:00 2021-07-13 21:40:00   \n",
       "2          SAN  DEN            2 2021-07-13 12:10:00 2021-07-13 15:30:00   \n",
       "3          SAN  DEN            2 2021-07-13 20:50:00 2021-07-14 00:05:00   \n",
       "4          SAN  HNL            2 2021-07-13 18:40:00 2021-07-13 21:40:00   \n",
       "5          SAN  HNL            2 2021-07-13 08:40:00 2021-07-13 11:45:00   \n",
       "...        ...  ...          ...                 ...                 ...   \n",
       "7580719    CMH  DCA            7 2022-01-09 12:24:00 2022-01-09 13:52:00   \n",
       "7580720    CMH  DCA            1 2022-01-10 12:24:00 2022-01-10 13:52:00   \n",
       "7580721    CMH  DCA            2 2022-01-11 12:24:00 2022-01-11 13:52:00   \n",
       "7580722    CMH  DCA            3 2022-01-12 12:24:00 2022-01-12 13:52:00   \n",
       "7580723    CMH  DCA            4 2022-01-13 12:24:00 2022-01-13 13:52:00   \n",
       "\n",
       "         ARR_DAY_OF_WEEK  \n",
       "0                      1  \n",
       "2                      1  \n",
       "3                      2  \n",
       "4                      1  \n",
       "5                      1  \n",
       "...                  ...  \n",
       "7580719                6  \n",
       "7580720                0  \n",
       "7580721                1  \n",
       "7580722                2  \n",
       "7580723                3  \n",
       "\n",
       "[7346524 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carrier_data[['ORIGIN', 'DEST', 'DAY_OF_WEEK', 'FL_DATE_LOCAL', 'FL_ARR_DATE_LOCAL', 'ARR_DAY_OF_WEEK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting so day_of_week index match\n",
    "carrier_data['ARR_DAY_OF_WEEK'] = carrier_data['ARR_DAY_OF_WEEK'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we make the day_of_week columns more reader-friendly\n",
    "day_of_week_translation = {1: 'Monday',\n",
    "                          2: 'Tuesday',\n",
    "                          3: 'Wednesday',\n",
    "                          4: 'Thursday',\n",
    "                          5: 'Friday',\n",
    "                          6: 'Saturday',\n",
    "                          7: 'Sunday'}\n",
    "\n",
    "carrier_data['DAY_OF_WEEK'].replace(day_of_week_translation, inplace=True)\n",
    "carrier_data['ARR_DAY_OF_WEEK'].replace(day_of_week_translation, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Takeoff Congestion Key\n",
    "carrier_data['takeoff-congestion-key'] = carrier_data['ORIGIN'] \\\n",
    "                        + carrier_data['DAY_OF_WEEK'].astype(str) \\\n",
    "                        + carrier_data['FL_DATE_LOCAL_ROUNDED'].dt.hour.astype(str).str.zfill(2)\n",
    "\n",
    "# Arrival Congestion Key\n",
    "carrier_data['arrival-congestion-key'] = carrier_data['DEST'] \\\n",
    "                        + carrier_data['ARR_DAY_OF_WEEK'].astype(str) \\\n",
    "                        + carrier_data['FL_ARR_DATE_LOCAL_ROUNDED'].dt.hour.astype(str).str.zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = carrier_data.groupby('takeoff-congestion-key')['FL_DATE_LOCAL_ROUNDED'].nunique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we create a new dataframe that holds data on congestion\n",
    "airport_congestion_by_hour = carrier_data.groupby('takeoff-congestion-key')['TAIL_NUM'].count()\n",
    "airport_congestion_by_hour = airport_congestion_by_hour.to_frame()\n",
    "airport_congestion_by_hour.reset_index(inplace=True)\n",
    "airport_congestion_by_hour.rename(columns={'TAIL_NUM': 'count_of_flights'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_congestion_by_hour['num_records'] = records\n",
    "airport_congestion_by_hour['avg-takeoff-congestion'] = airport_congestion_by_hour['count_of_flights'] / airport_congestion_by_hour['num_records']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we calculate landing congestion\n",
    "airport_arrival_congestion_by_hour = carrier_data.groupby('arrival-congestion-key')['TAIL_NUM'].count()\n",
    "airport_arrival_congestion_by_hour = airport_arrival_congestion_by_hour.to_frame()\n",
    "airport_arrival_congestion_by_hour.reset_index(inplace=True)\n",
    "airport_arrival_congestion_by_hour.rename(columns={'TAIL_NUM': 'count_of_flights_arriving'}, inplace=True)\n",
    "\n",
    "arr_records = carrier_data.groupby('arrival-congestion-key')['FL_ARR_DATE_LOCAL_ROUNDED'].nunique().tolist()\n",
    "\n",
    "airport_arrival_congestion_by_hour['num_arr_records'] = arr_records\n",
    "airport_arrival_congestion_by_hour['avg-arrival-congestion'] = airport_arrival_congestion_by_hour['count_of_flights_arriving'] / airport_arrival_congestion_by_hour['num_arr_records']\n",
    "\n",
    "airport_congestion_by_hour.drop(columns=['count_of_flights', 'num_records'], inplace=True)\n",
    "airport_arrival_congestion_by_hour.drop(columns=['count_of_flights_arriving', 'num_arr_records'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally we can combine the two into a congestion dataframe\n",
    "airport_congestion_by_hour = pd.merge(airport_congestion_by_hour, airport_arrival_congestion_by_hour,\n",
    "                                      left_on='takeoff-congestion-key', right_on='arrival-congestion-key')\n",
    "airport_congestion_by_hour.drop(columns=['arrival-congestion-key'], inplace=True)\n",
    "\n",
    "airport_congestion_by_hour.rename(columns={\"takeoff-congestion-key\": \"congestion-key\"}, inplace=True)\n",
    "# airport_congestion_by_hour.to_csv('data/prepared/airport_congestion_by_hour.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging in Congestion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we add congestion data to our main dataframe\n",
    "carrier_data = pd.merge(carrier_data, airport_congestion_by_hour, left_on='takeoff-congestion-key', right_on='congestion-key')\n",
    "# updating key\n",
    "airport_congestion_by_hour = airport_congestion_by_hour.add_prefix('dest-')\n",
    "\n",
    "# Now data on the congestion conditions of the airport where the flight is arriving\n",
    "carrier_data = pd.merge(carrier_data, airport_congestion_by_hour, left_on='arrival-congestion-key', right_on='dest-congestion-key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximity to Holidays\n",
    "Anyone who has ever traveled by plane knows that delays seem to be most prevelant around the holidays. That's why one last feature we want to engineer is some sort of proximity to holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "date_range = pd.date_range(start='2021-01-01', end='2025-12-31')\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start=date_range.min(), end=date_range.max(), return_name=True)\n",
    "holidays.reset_index(name='holiday').rename(columns={'index':'date'})\n",
    "holidays = holidays.to_frame()\n",
    "holidays.reset_index(inplace=True)\n",
    "holidays.columns = ['holiday_date', 'holiday_name']\n",
    "\n",
    "# holidays.to_csv('data/prepared/holidays.csv', index=False)\n",
    "\n",
    "# carrier_data['holiday-key'] = carrier_data['FL_DATE_LOCAL'].dt.date.astype(str)\n",
    "# carrier_data['holiday-key'] = pd.to_datetime(carrier_data['holiday-key'])\n",
    "\n",
    "carrier_data.sort_values('FL_DATE_LOCAL', inplace=True)\n",
    "carrier_data = pd.merge_asof(carrier_data, holidays, left_on='FL_DATE_LOCAL', right_on='holiday_date',\n",
    "                       direction='nearest', tolerance=pd.Timedelta(days=7))\n",
    "\n",
    "carrier_data['days-from-holiday'] = (carrier_data['FL_DATE_LOCAL'] - carrier_data['holiday_date']).dt.days\n",
    "carrier_data['days-from-holiday'] = carrier_data['days-from-holiday'].astype(str)\n",
    "carrier_data['days-from-specific-holiday'] = carrier_data['holiday_name'] + '_' + carrier_data['days-from-holiday'].astype(str)\n",
    "\n",
    "# Cleaning up the results a bit\n",
    "carrier_data['days-from-specific-holiday'].fillna('no-close-holiday', inplace=True)\n",
    "\n",
    "carrier_data['holiday'] = carrier_data.loc[carrier_data['days-from-holiday'] == 0, 'holiday_name']\n",
    "carrier_data['holiday'].fillna('Not a Holiday', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again, we clean up columns we don't need anymore\n",
    "carrier_data.drop(columns=['holiday_date', 'holiday_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data Before We Filter it Down\n",
    "Our next steps will involve filtering down data. Before we do that, we'll save our data out to a file so we can use it for graphing and vizualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_data.to_csv('data/prepared/data_for_graphing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time of day takoff & landing\n",
    "\n",
    "Lastly, we will create a continuous variable that quantifies when throughout the day that flight takes off. This is to account that it's possible for delays to be more prevalent at certain points in the day. To do this, we'll create a variable that measures a flight's distance from midnight in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_data['takeoff-mins-from-midnight'] = ((pd.to_datetime(carrier_data['CRS_DEP_TIME'])\n",
    "                                               - pd.to_datetime(carrier_data['CRS_DEP_TIME']).dt.normalize()) \\\n",
    "                                              / pd.Timedelta('1 minute')).astype(int)\n",
    "\n",
    "carrier_data['CRS_ARR_TIME'] = carrier_data['CRS_ARR_TIME'].replace({'24:00':'00:00'})\n",
    "carrier_data['landing-mins-from-midnight'] = ((pd.to_datetime(carrier_data['CRS_ARR_TIME'])\n",
    "                                               - pd.to_datetime(carrier_data['CRS_ARR_TIME']).dt.normalize()) \\\n",
    "                                              / pd.Timedelta('1 minute')).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering for Airports & Airlines Relevant to Business Case\n",
    "The MVP calls for us to support flights originating from major US airports and 8 major airlines. So we filter down our data for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create list of relevant aiports based on business case\n",
    "relevant_airports = ['ATL', 'DFW', 'DEN', 'ORD', 'LAX', 'CLT', 'LAS', 'PHX', \n",
    "                     'MCO', 'SEA', 'MIA', 'IAH', 'JFK', 'FLL', 'EWR', 'SFO', 'MSP', 'DTW',\n",
    "                     'BOS', 'SLC', 'PHL', 'BWI', 'TPA', 'SAN', 'MDW', 'LGA', 'BNA', 'IAD',\n",
    "                     'DAL', 'DCA', 'PDX', 'AUS', 'HOU', 'HNL', 'STL', 'RSW', 'SMF', 'MSY',\n",
    "                     'SJU', 'RDU', 'OAK', 'MCI', 'CLE', 'IND', 'SAT', 'SNA', 'PIT', 'CVG',\n",
    "                     'CMH', 'PBI', 'JAX', 'MKE', 'ONT', 'ANC', 'BDL', 'OGG', 'OMA', 'MEM',\n",
    "                     'BOI', 'RNO', 'CHS', 'OKC']\n",
    "\n",
    "# Create list of relevant IATA airline designators based on business case\n",
    "relevant_airlines = ['WN', # Southwest\n",
    "                     'DL', # Delta\n",
    "                     'OO', # SkyWest\n",
    "                     'AA', # American Airlines\n",
    "                     'UA', # United Airlines\n",
    "                     'B6', # JetBlue\n",
    "                     'AS', # Alaska Airlines\n",
    "                     'NK', # Spirit Airlines\n",
    "                    ]\n",
    "\n",
    "# Filter Dataframe to include only relevant airlines & airports\n",
    "airport_filter = '|'.join(relevant_airports)\n",
    "airline_filter = '|'.join(relevant_airlines)\n",
    "\n",
    "carrier_data = carrier_data[carrier_data['ORIGIN'].str.contains(airport_filter)]\n",
    "carrier_data = carrier_data[carrier_data['OP_CARRIER'].str.contains(airline_filter)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Cutting Down The Data\n",
    "For the sake of efficiency, we can further filter down our data to include only destinations that are at least somewhat frequently traveled. If, out of 14 million flights, a destination was visited fewer than 1000 times we'll cut that flight. Also, flight numbers that only appear less than 100 times in our data can be pruned as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_data = carrier_data.groupby('DEST').filter(lambda x: len(x) > 1000)\n",
    "carrier_data['flight-number'] = carrier_data['MKT_CARRIER'] + carrier_data['MKT_CARRIER_FL_NUM'].astype(str)\n",
    "carrier_data = carrier_data.groupby('flight-number').filter(lambda x: len(x) > 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging in Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to pull up the latitude and longitude coordinates for all our airports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all-in-all we're dealing with 124 locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many locations and dates we have to look up in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are about 90k unique location and date parings for us to look up weather for. We'll be using the WeatherAPI.com api to pull this data in.\n",
    "\n",
    "The API limits us to making a call for just 30 days at a time. We can also look up just one location at a time.\n",
    "\n",
    "Our approach here is to loop over every location in 30-day increments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = datetime.strptime('2021-06-01', '%Y-%m-%d').date()\n",
    "# end = datetime.strptime('2022-07-31', '%Y-%m-%d').date()\n",
    "\n",
    "# current_date = start\n",
    "# date_list = []\n",
    "\n",
    "# while current_date <= end:\n",
    "#     date_list.append(current_date)\n",
    "#     current_date_plus_30 = current_date + timedelta(days=30)\n",
    "#     date_list.append(current_date_plus_30)\n",
    "#     current_date = current_date_plus_30 + timedelta(days=1)\n",
    "\n",
    "# for i in range(0,len(date_list)):\n",
    "#     date_list[i] = date_list[i].strftime('%Y-%m-%d')\n",
    "    \n",
    "# start_dates = []\n",
    "# end_dates = []\n",
    "# for i in range(0,len(date_list)):\n",
    "#     if i%2 == 0:\n",
    "#         start_dates.append(date_list[i])\n",
    "#     else:\n",
    "#         end_dates.append(date_list[i])\n",
    "        \n",
    "# origins = list(carrier_data['origin-lat-long'].unique())\n",
    "# destinations = list(carrier_data['dest-lat-long'].unique())\n",
    "# lat_long_list = list(set(origins+destinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_keys(path):\n",
    "#     with open(path) as f:\n",
    "#         return json.load(f)\n",
    "    \n",
    "# keys = get_keys(\"C:/Users/Robert/.secret/weather_api.json\")\n",
    "# history_data_url = 'http://api.weatherapi.com/v1/history.json'\n",
    "# api_key = keys['api_key']\n",
    "\n",
    "# r = requests.get(history_data_url + '?key=' + api_key + '&q=' + location + '&dt=' + start_dates[i] + '&end_dt=' + end_dates[i])\n",
    "# d = json.loads(r.text)\n",
    "\n",
    "# weather_latlong = []\n",
    "# weather_dates = []\n",
    "# weather_maxtemp_c = []\n",
    "# weather_mintemp_c = []\n",
    "# weather_avgtemp_c = []\n",
    "# weather_totalprecip_mm = []\n",
    "# weather_avgvis_km = []\n",
    "# weather_maxwind_kph = []\n",
    "# weather_avghumidity = []\n",
    "\n",
    "# for location in lat_long_list:\n",
    "#     print('Working on {}'.format(location))\n",
    "#     for i in range(0, len(start_dates)):\n",
    "#         r = requests.get(history_data_url + '?key=' + api_key + '&q=' + location + '&dt=' + start_dates[i] + '&end_dt=' + end_dates[i])\n",
    "#         d = json.loads(r.text)\n",
    "#         for j in range(0,31):\n",
    "#             weather_latlong.append(location)\n",
    "#             weather_dates.append(d['forecast']['forecastday'][j]['date'])\n",
    "#             weather_maxtemp_c.append(d['forecast']['forecastday'][j]['day']['maxtemp_c'])\n",
    "#             weather_mintemp_c.append(d['forecast']['forecastday'][j]['day']['mintemp_c'])\n",
    "#             weather_avgtemp_c.append(d['forecast']['forecastday'][j]['day']['avgtemp_c'])\n",
    "#             weather_totalprecip_mm.append(d['forecast']['forecastday'][j]['day']['totalprecip_mm'])\n",
    "#             weather_avgvis_km.append(d['forecast']['forecastday'][j]['day']['avgvis_km'])\n",
    "#             weather_maxwind_kph.append(d['forecast']['forecastday'][j]['day']['maxwind_kph'])\n",
    "#             weather_avghumidity.append(d['forecast']['forecastday'][j]['day']['avghumidity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_df = pd.DataFrame(\n",
    "#     {'lat-long': weather_latlong,\n",
    "#      'date': weather_dates,\n",
    "#      'maxtemp': weather_maxtemp_c,\n",
    "#      'mintemp': weather_mintemp_c,\n",
    "#      'avgtemp': weather_avgtemp_c,\n",
    "#      'totalprecip': weather_totalprecip_mm,\n",
    "#      'avgvis': weather_avgvis_km,\n",
    "#      'maxwind': weather_maxwind_kph,\n",
    "#      'avghumidity': weather_avghumidity\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_df.to_csv('data/downloaded/weather-data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('data/downloaded/weather-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, we need to create keys for matching the weather data to locations and dates\n",
    "carrier_data['weather-key'] = carrier_data['origin-lat-long'] + carrier_data['FL_DATE_LOCAL'].dt.date.astype(str)\n",
    "carrier_data['dest-weather-key'] = carrier_data['dest-lat-long'] + carrier_data['FL_ARR_DATE_LOCAL'].dt.date.astype(str)\n",
    "weather_df['weather-key'] = weather_df['lat-long'] + weather_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a key to prepare merging in daily weather data for each airport\n",
    "carrier_data = carrier_data.merge(weather_df, on='weather-key')\n",
    "weather_df = weather_df.add_prefix('dest-')\n",
    "carrier_data = carrier_data.merge(weather_df, on='dest-weather-key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Once again, we clean up columns we don't need anymore\n",
    "carrier_data.drop(columns=['FL_DATE', 'MKT_CARRIER_FL_NUM', 'OP_CARRIER_FL_NUM', 'FL_ARR_DATE_REL_ORIGIN',\n",
    " 'date', 'dest-lat-long_y', 'dest-date'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving out the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_data.to_csv('data/prepared/cleaned_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
